[Link](https://www.cs.cmu.edu/~sandholm/cs15-888F21/lecture1.pdf)
***
##### 1 - Introduction

**Q:**  Why study multi-step imperfect-information games?

**A:** Most real-world games are incomplete-information games with sequential (or simultaneous) moves. 

Examples:
- Negotiation
- Multi-stage auctions
- Sequential auctions of multiple items
- Card games
- etc.

>Techniques for perfect-information games (ex. checkers, chess, and go) don't apply to incomplete-information games.

**Why?** 
 - Private information
 - Need to understand signals and how other players will interpret signals
 - Need to understand deception


##### 2 - Terminology 

- <span style="color:cyan"><i>Agent</i></span> (or *player*)
<br/>
- <span style="color:cyan"><i>Action</i></span> (or *move*) - choice that the agent can make at a point in the game.
<br/>
* <span style="color:cyan"><i>Strategy</i></span> ($s_i$) - mapping from history to actions.
$\quad\quad\quad\quad\quad\quad$(to the extent the agent can distinguish a history)
<br/>
* <span style="color:cyan"><i>Strategy set</i></span> ($S_i$) - strategies available to the agent.
<br/>
* <span style="color:cyan"><i>Strategy profile</i></span> ($s_1, s_2 , ..., s_{|A|}$) - one strategy for each agent.
<br/>
- <span style="color:cyan"><i>Utility</i></span> ($u_i = u_i(s_1, s_2 , ..., s_{|A|})$) - <br/>Represents the motivations of the agents, maps outcomes to reals with the property that a higher number implies that outcome is more preferred. <br/>
*Note* -  Utility seems to be closely related to *reward* in RL? <br/>
An agent's utility is only determined after all agents, including nature, have chosen their strategy.
<br/>
* <span style="color:cyan"><i>Nature</i></span> - pseudo agent that is used to model uncertainty.
*Note* - Nature seems to be closely related to *environment* in RL.


##### 3 - Agenthood

>Agents seek to maximize their expected utility: <br/>
$\quad\max_{\textrm{strategy}} \: \sum_{\textrm{outcome}} \: p(\textrm{outcome} \: |  \: \textrm{strategy}) \: u_i(\textrm{outcome})$
<br/>

- Utility functions are scale-invariant. <br/>
	*Note* - We are only interested in the relative utility between strategies, so scaling them all by a linear transform does not change our rankings of actions.
<br/>


- The utility function is up to the agent and can be used to model an agent's risk attitude.

	*Example:* 
	Consider the two lotteries:
	Lottery 1 - 100% chance of winning $0.5M
	Lottery 2 - 50% chance off winning $1M
	
	The lotteries have equal expected values however agents may prefer one to the other depending on their willingness to take risk.

* Often times, in game theory, only expected payoff or expected value (EV) is considered.



##### 4 - Game representations

- <span style="color:cyan"><i>Extensive form</i></span> - game tree form in which agents are nodes and their actions are edges.
<br/>

- <span style="color:cyan"><i>Matrix form</i></span> - Player 1's strategy is along the rows, Player 2's strategy is along the columns, each cell denotes the utility for each player given the action pair.



##### 5 - Dominant strategy "equilibrium"

* <span style="color:cyan"><i>Best response</i></span> ($s_i^*$) - $u_i(s_i^*, s_{-i}) \geq u_i(s_i', s_{-i}), \; \; \forall \: s_i'$ . 
	The strategy that yields the highest utility in response to the opposing player's strategy, $s_{-i}$ .
<br/>

* <span style="color:cyan"><i>Dominant strategy</i></span> ($s_i^*$) -  $s_i^*$ is the best response for all opponent strategies, $s_{-i}$
	The agent's best strategy does not depend on its opponent's strategy.
<br/>

* <span style="color:cyan"><i>Dominant strategy "equilibrium"</i></span> - 
	Strategy profile in which each agent has chosen its dominant strategy.
	Therefore, no agent has motivation to change their strategy.

*Note* - dominant strategies don't always exist.


##### 6 - Nash equilibrium

* <span style="color:cyan"><i>Nash equilibrium</i></span> - 
		No player has incentive to deviate from their strategy so long as their opponents do not deviate either. For every agent $i$,   $u_i(s_i^*, s_{-i}) \geq u_i(s_i',s_{-i}), \; \; \forall s_i'$
<br/>

>Nash equilibrium is a subset of the dominant strategy equilibrium. Dominant strategy equilibrium holds for all opponent strategies; whereas, Nash equilibrium only holds for a fixed opponent strategy. 
>
> _i.e._ I don't change if you don't change, and vice versa.

<br/>

Nash equilibrium may not exist or be unique for all games. (ex. Battle of the Sexes)

* <span style="color:red"><b>Theorem:</b></span> **Existence of (pure-strategy) Nash equilibria**
	Any finite game, where each action node is alone in its information set is dominance solvable by backward induction.
	
	*Note* - an agent is "alone in its information set" if at every point in the game, the agent knows what moves have been played so far.
	
	**Proof:** Multi-player minimax search


##### 7 - Mixed-strategy Nash equilibrium (rock-paper-scissors)

![[rock-paper-scissors.jpg|500]]

- <span style="color:cyan"><i>Bayesian game</i></span> - game in which players have incomplete information.
	In the case of rock-paper-scissors, the agents don't have knowledge of the other's move.
<br/>

- <span style="color:cyan"><i>Bayes-Nash equilibrium</i></span> - Nash equilibrium of a Bayesian game.
	A strategy profile that maximizes the _expected_ payoff for each player given their beliefs and strategies played by others.
<br/>

- <span style="color:cyan"><i>Mixed strategy</i></span> - the agent's strategy is a weighted mixture of pure strategies.
	Rock-paper-scissors has a _symmetric_ mixed strategy Nash equilibrium where all the agents have the same strategy.
<br/>

- Existence:
	<span style="color:red"><b>Theorem:</b></span> Every finite player, finite strategy game has at least one Nash equilibrium if we admit mixed-strategy equilibria as well as pure. <span style="color:green">[1950-Nash]</span>.
<br/>

- Complexity of finding a Nash equilibrium in a normal form game:
	- 2-player 0-sum: polytime with LP
	- 2-player games:
		- PPAD-complete <span style="color:green">[2009-Chen] [2005-Abbot] [2006-Daskalakis]</span>
		- NP-complete to find an approx. _good_ Nash equilibrium <span style="color:green">[2008-Conitzer]</span>
	- 3-player games are FIXP-complete <span style="color:green">[2007-Etessami]</span>
