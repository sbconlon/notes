[Link](https://www.cs.cmu.edu/~sandholm/cs15-888F21/L03_regret.pdf)
***

##### 0 - Introduction

-  How can we 'learn' from repeatedly playing a game? 
- How does this _dynamic_ concept of learning relate to the _static_ concept of game-theoretic equilibria?
<br/>

##### 1 - Hindsight rationality and $\Phi$-Regret

- Suppose we have one player in a game, let:
	- $\mathcal{X}$ - set of available strategies.
	- $x^t$ - strategy played at timestep $t$.
	- Player receives some feedback after each play through, could be a gradient of the utility or the utility itself.
	- $x^{t+1}$ - some 'better' strategy it has formulated based on the feedback.
<br/>

-  <span style="color:cyan"><i>Hindsight rationality</i></span> - a player has "learnt" to play the game when looking back at the history of play, they cannot think of any transformation: $\phi: \mathcal{X} \rightarrow \mathcal{X}$ of their strategies that when applied to the whole history of play would've yielded a strictly better utility to the player.
<br/>

- <span style="color:cyan"><i>$\Phi$-regret minimizer</i></span> - model for a decision maker that repeatedly interacts with a black-box, given a set $\mathcal{X}$ of points and a set $\Phi$ of linear transformations $\phi: \mathcal{X} \rightarrow \mathcal{X}$. <br/><br/>
	- At each time $t$, the regret minimizer interacts with the environment through two operations: <br/><br/>
		- $\text{NextStrategy}$ - regret minimizer returns an element $x^t \in \mathcal{X}$ . <br/><br/>
		- $\text{ObserveUtility}$ ($\ell^t$)  - provides feedback to the minimizer from the environment, $\ell^t \: : \: \mathcal{X} \rightarrow \mathbb{R}$, based on how good the last strategy $x^t$ was. <br/><br/>

<br/>


- <span style="color:cyan"><i>Cumulative $\Phi$-regret</i></span> - regret minimizer's quality metric, its goal is to guarantee that $\Phi$-regret grows asymptotically sublinearly as time $T$ increases.
  $$R_\Phi^T := \max_{\hat{\phi} \in \Phi} \biggl\{ \sum_{t=1}^T \biggr( \ell^t(\hat{\phi}(x^t)) - \ell^t(x^t) \biggl) \biggr\}$$    
<br/>

- $\text{NextStrategy}$ and $\text{ObserveUtility}$ alternate: the minimizer presents a new strategy $x^t$ to the black box and receives a new utility function back $\ell^t$ from the black box. It then uses this feedback to formulate a new strategy $x^{t+1}$ and so on.
<br/>

- The regret minimizer's decision making is online in the sense that its strategy depends on its past strategies and environmental feedback (observed utility functions).
<br/>

###### 1.1 - Some Notable Choices for the Set of Transformations $\Phi$ Regret

- What set of transitions should the agent consider for its $\Phi$ ? The size of $\Phi$ defines the agent's rationality.
<br/>

- Possible choices of $\Phi$ : <br/><br/>
	- $\Phi$ = <span style="color:cyan"><i>Swap regret</i></span> - set of all mappings from $\mathcal{X}$ to $\mathcal{X}$.
		- Maximum size of $\Phi$ and therefore the highest level of hindsight rationality. <br/><br/>
		
	- $\Phi$ = <span style="color:cyan"><i>Internal regret</i></span> - set of all single-point deviations.
		Each strategy maps directly to a different strategy. 
		$$\Phi = \{\phi_{a\rightarrow b}\}_{a,b\in\Phi}$$
		where:
		$$\phi_{a\rightarrow b} : x \rightarrow \begin{cases} x, & \text{if}\ x \neq a \\ b, & \text{if}\ x = a \end{cases}$$
	>When all agents in a multiplayer general-sum game use internal or swap regret, their empirical frequency of play converges to a _correlated equilibrium_. <br/>
	
	- $\Phi$ = <span style="color:cyan"><i>Trigger deviation functions</i></span>
		 $\Phi$-regret is efficiently bounded with a polynomial dependence on the size of the game tree.
	> When all agents in a multiplayer general-sum game use trigger deviation functions, their empirical frequency of play converges to an _extensive-form correlated equilibrium_ <br/>
	
	- $\Phi$ = <span style="color:cyan"><i>External regret</i></span> (constant transforms) - requires that the player not regret substituting all of the strategies they played with the same strategy $a$.
	> When all agents in a multiplayer general-sum game use external regret, their empirical frequency of play converges to a _coarse correlated equilibrium_. <br/>

    >  When all agents in a two-player zero-sum game use external regret, their average strategies converge to _Nash equilibrium_.


###### 1.2 - A Very Important Special Case: Regret Minimization

- <span style="color:cyan"><i>Regret minimizer</i></span> (external regret minimizer) - special case of the $\Phi$-regret minimizer where $\Phi$ is chosen to be the set of constant transforms:
	$$\Phi^\text{Const}:=\{\phi_\hat{x}:x \rightarrow \hat{x}\}_{\hat{x}\in \mathcal{X}}$$
	External regret:
	$$R^T := \max_{\hat{x} \in \mathcal{X}} \biggl\{\sum_{t=1}^T \Bigl(\ell^t(\hat{x})-\ell^t(x^t) \Bigr) \biggr\}$$<br/>
	Goal is for the cumulative regret $R^T$ to grow sublinearly in $T$.
<br/>

- Online linear optimization asserts sublinear regret for convex and compact domain $\mathcal{X}$, on the order $R^T=O(\sqrt{T})$.
<br/>

- External regret minimization guarantees:
	- Nash equilibrium in two-player zero-sum games.
	- Coarse correlated equilibrium in multiplayer general-sum games
	- Best responses to static stochastic opponents in multiplayer general-sum games.
	- etc.

###### 1.3 From Regret Minimization to $\Phi$-Regret Minimization

_Note: I don't understand this section_

- There exits a construction such that $\Phi$-regret minimization reduces to regret minimization. <span style="color:green">[2008-Gordon]</span>
<br/>

- <span style="color:red"><b>Theorem:</b></span> <br/><br/>
	
	- Let $\mathcal{R}$ be a deterministic regret minimizer with external regret and every $\phi \in \Phi$ admits fixed points $\phi(x) = x \in \mathcal{X}$. <br/><br/>
	
	- A $\Phi$-regret minimizer, $\mathcal{R}_\Phi$ , can be constructed from $\mathcal{R}$ : <br/><br/>
		
		- Each $\mathcal{R}_\Phi$ call to $\text{NextStrategy}$ calls $\mathcal{R}$ 's $\text{NextStrategy}$ to get the next transform, $\phi^t$ , which is then used to compute the next strategy $x^t = \phi^t(x^t)$. <br/><br/>
		
		- Each $\mathcal{R}_\Phi$ call to $\text{ObserveUtility}(\ell^t)$ constructs a linear utility function, $L^t : \phi \rightarrow \ell^t(\phi(x^t))$, that is then passed to $\mathcal{R}$ 's  $\text{ObserveUtility}(L^t)$. <br/><br/>
		
	- This $\Phi$-regret minimizer shares the same cumulative regret as the external regret minimizer, $R_\Phi^T=R^T$. Therefore the regret accumulated by $R_\Phi$ grows sublinearly. <br/><br/> 


- Proof: <br/><br/>
	- $\mathcal{R}$ outputs transformations $\phi^1, \phi^2, ... \in \Phi$ and receives utilities $\phi \rightarrow L^1(x^1)), \: \phi \rightarrow L^2(x^2)), \: ...$ <br/><br/>
	- Cumulative regret for $\mathcal{R}$, then:
		$$R^T=\max_{\hat{\phi} \in \Phi} \biggl\{\sum_{t=1}^T \Bigl(\ell^t(\hat{\phi}(x^t))-L^t(x^t)) \Bigr) \biggr\}$$
		<br/>$L^t(x^t)=\ell^t(\phi^t(x^t))$ and $\phi^t(x^t)=x^t$, thus
		$$R^T=\max_{\hat{\phi} \in \Phi} \biggl\{\sum_{t=1}^T \Bigl(\ell^t(\hat{\phi}(x^t))-\ell^t(x^t) \Bigr) \biggr\}$$
		<br/>which equals $R_\Phi^T$ cumulative regret.
<br/>

##### 2. Applications of regret minimization

###### 2.1 - Learning a Best Response Against Stochastic Opponents

- Consider the game setup: <br/><br/>
	- Playing a repeated $n$-player general-sum game with multilinear utilities. <br/><br/>
	- Players $i=1, ..., n-1$ play stochastically:
		At each each $t$ they independently sample a strategy $x^{(i), t}\in \mathcal{X}^{(i)}$ from a fixed distribution. <br/>
		$x^{(i),t}$ - the strategy played by Player $i$  at time $t$. <br/>
		$\bar{x}^{(i)}$ - the average strategy played by Player $i$. <br/><br/>
	- Player $n$ is the learning agent, it picks strategies according to an algorithm that guarantees sublinear external regret. <br/><br/>
	- Player $n$ feedback function at time $t$, defined as
		$\ell^t := \mathcal{X}^{(n)} \ni  x^{(n)} \: \rightarrow \: u^{(n)}\bigl(x^{(1),t}, ..., x^{(n-1),t}, x^{(n)} \bigr)$ 



<br/>

- The average strategy played by Player $n$ converges to a best response:
	$$\frac{1}{T} \sum_{t=1}^T x^{(n),\, t} \rightarrow \underset{\hat{x}^{(n)} \in \mathcal{X}^{(n)}}{\mathrm{arg\,max}} \: \biggl\{ u^{(n)} \bigl( \bar{x}^{(1)}, \: ..., \: \bar{x}^{(n-1)}, \: \hat{x}^{(n)} \bigr)\biggr\}$$ 
###### 2.2 - Self-Play Convergence to Bilinear Saddle Points <br/> (such as Nash equilibrium in a two-player zero-sum game)

- Regret minimization can be used to converge to bilinear saddle points.
<br/>

- Bilinear saddle points are solutions of the form: <br/>
	$$\max_{x \in \mathcal{X}} \min_{y \in \mathcal{Y}} \: x^\intercal Ay$$
	<br/>Where $\mathcal{X}$ and $\mathcal{Y}$ are convex and compact sets. <br/>

- This type of optimization problem is pervasive in game-theory.
	- _Example:_ Computation of Nash equilibria in two-player zero-sum games.
		- $\mathcal{X}$ - strategy space for Player 1
		- $\mathcal{Y}$ - strategy space for Player 2
		- $A$ - payoff matrix for Player 1
<br/>

- Use self play between regret minimizers to converge to bilinear saddle-points: <br/><br/>
	- Let each player be a regret minimizer, $\mathcal{R}_\mathcal{X}$ and $\mathcal{R}_\mathcal{Y}$. <br/><br/>
	- At each time $t$, each minimizer outputs a strategy $x^t$ and $y^t$, and receives feedback:
		$$\ell_\mathcal{X}^t : x \rightarrow (Ay^t)^\intercal x \;\;\;\;\;\;\; \ell_\mathcal{Y}^t : y \rightarrow -(A^\intercal x^t)^\intercal y$$
#

- <span style="color:cyan"><i>Saddle point gap</i></span> ($\gamma$) - difference between the saddle points produced by the average strategies $(\hat{x}, \hat{y})$  of the regret minimizers up to any time $T$ and a given pair of strategies $(x,y)$, used to measure convergence.
	$$0 \leq \gamma\,(x,y) := \bigl(\max_{\hat{x}\in \mathcal{X}} \{\hat{x}^\intercal Ay\} - x^\intercal Ay \bigr) \: + \: \bigl( x^\intercal Ay - \min_{\hat{y}\in \mathcal{Y}} \{x^\intercal A\hat{y}\}\bigr)$$$$\gamma\,(x,y)=\max_{\hat{x}\in \mathcal{X}}\{\hat{x}^\intercal Ay\} - \min_{\hat{y}\in \mathcal{Y}} \{x^\intercal A\hat{y}\}$$

#

- <span style="color:red"><b>Theorem:</b></span> Let $R_\mathcal{X}^T$ and $R_\mathcal{Y}^T$ be the sublinear cumulative regret of the minimizers, up to time $T$. And, $\hat{x}^T$ and $\hat{y}^T$ be the average of the strategies. Then the saddle point gap satisfies:
	$$\gamma\,(\hat{x}^T, \hat{y}^T) \leq \frac{R_\mathcal{X}^T + R_\mathcal{Y}^T}{T} \rightarrow 0 \; \; \; \text{as} \; \; T \rightarrow 0$$
(See the lecture notes for the proof.)